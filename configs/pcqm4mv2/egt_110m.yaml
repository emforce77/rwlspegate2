scheme: pcqm4mv2
model_name: egt_110m
distributed: false         # Set = true for multi-gpu
batch_size: 512            # For 8 GPUs: 512//8=64
model_height: 30
node_width: 768
edge_width: 64
num_heads: 32
num_epochs: 1000
max_lr: 8.0e-05
attn_dropout: 0.3
lr_warmup_steps: 240000
lr_total_steps: 1000000
node_ffn_multiplier: 1.0
edge_ffn_multiplier: 1.0
upto_hop: 16
dataloader_workers: 1      # For multi-process data fetch
scale_degree: true
num_virtual_nodes: 4
svd_random_neg: true
