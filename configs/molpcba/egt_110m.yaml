scheme: molpcba
model_name: egt_110m
distributed: false         # Set = true for multi-gpu
batch_size: 16             # For 8 GPUs: 16//8=2
model_height: 30
node_width: 768
edge_width: 64
num_heads: 32
num_epochs: 1000
max_lr: 0.0001
attn_dropout: 0.3
lr_warmup_steps: 20000
lr_total_steps: 200000
node_ffn_multiplier: 1.0
edge_ffn_multiplier: 1.0
upto_hop: 16
dataloader_workers: 1      # For multi-process data fetch
scale_degree: true
num_virtual_nodes: 4
svd_random_neg: true
pretrained_weights_file: models/pcqm4mv2/egt_110m/checkpoint/model_state
                           # ^ For transfer learning from PCQM4Mv2